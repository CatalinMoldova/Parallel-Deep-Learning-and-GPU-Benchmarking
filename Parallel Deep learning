{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee5d12a-a485-4b78-a0d4-dc4b88b7524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pytorch-cifar directory already exists, skipping clone\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "Number of GPUs: 4\n",
      "GPU 0: NVIDIA L4\n",
      "GPU 1: NVIDIA L4\n",
      "GPU 2: NVIDIA L4\n",
      "GPU 3: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if pytorch-cifar directory exists\n",
    "if not os.path.exists('pytorch-cifar'):\n",
    "    print(\"Cloning pytorch-cifar repository...\")\n",
    "    !git clone https://github.com/kuangliu/pytorch-cifar.git\n",
    "else:\n",
    "    print(\"✅ pytorch-cifar directory already exists, skipping clone\")\n",
    "\n",
    "# Copy resnet.py if needed\n",
    "if not os.path.exists('resnet.py') and os.path.exists('pytorch-cifar/models/resnet.py'):\n",
    "    !cp pytorch-cifar/models/resnet.py .\n",
    "    print(\"✅ Copied resnet.py\")\n",
    "\n",
    "# Import ResNet model\n",
    "sys.path.append('./pytorch-cifar')\n",
    "from models import resnet\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51758eec-b9dc-4ef1-8926-bcb8fec98171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data...\n",
      "Training batches: 391\n",
      "Testing batches: 100\n"
     ]
    }
   ],
   "source": [
    "def get_dataloaders():\n",
    "    \"\"\"\n",
    "    Create DataLoaders for CIFAR10 with specified transformations\n",
    "    \"\"\"\n",
    "    # Training transformations\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.4914, 0.4822, 0.4465),\n",
    "            std=(0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Testing transformations\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.4914, 0.4822, 0.4465),\n",
    "            std=(0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train\n",
    "    )\n",
    "    \n",
    "    trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform_test\n",
    "    )\n",
    "    \n",
    "    testloader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "# Create dataloaders\n",
    "print(\"==> Preparing data...\")\n",
    "trainloader, testloader = get_dataloaders()\n",
    "print(f\"Training batches: {len(trainloader)}\")\n",
    "print(f\"Testing batches: {len(testloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a814b3f6-65d8-4342-854d-d000673edb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "==> Building model...\n",
      "Let's use 4 GPUs with DataParallel!\n",
      "Model setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create ResNet18 model\n",
    "print(\"==> Building model...\")\n",
    "net = resnet.ResNet18()\n",
    "\n",
    "# Wrap with DataParallel for multi-GPU training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Let's use {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "    net = nn.DataParallel(net)\n",
    "else:\n",
    "    print(\"Using single GPU\")\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[150, 250],\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "print(\"Model setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788ec547-2bd0-4126-bcf7-98eef9f4596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, trainloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(trainloader)} | '\n",
    "                  f'Loss: {train_loss/(batch_idx+1):.3f} | '\n",
    "                  f'Acc: {100.*correct/total:.2f}% ({correct}/{total})')\n",
    "    \n",
    "    return train_loss / len(trainloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def test(model, testloader, criterion, device):\n",
    "    \"\"\"Evaluate on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_acc = 100. * correct / total\n",
    "    avg_loss = test_loss / len(testloader)\n",
    "    print(f'Test Loss: {avg_loss:.3f} | Test Acc: {test_acc:.2f}% ({correct}/{total})')\n",
    "    \n",
    "    return avg_loss, test_acc\n",
    "\n",
    "print(\"Training functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af1b3e1-2d5a-499e-b01b-4a001ffdcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Training transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2023, 0.1994, 0.2010)\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Testing transformations (if needed)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2023, 0.1994, 0.2010)\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a084ec-00e9-4fd4-91b1-5c3429b564cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the measuring functions\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Part 1\n",
    "def measure_single_gpu_training(batch_sizes=[32, 128, 512]):\n",
    "    \"\"\"\n",
    "    Measure training time for different batch sizes on single GPU\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Create fresh model for each run\n",
    "            net = resnet.ResNet18()\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            net = net.to(device)\n",
    "            \n",
    "            # Create dataloader with current batch size\n",
    "            trainset = torchvision.datasets.CIFAR10(\n",
    "                root='./data', train=True, download=True,\n",
    "                transform=transform_train\n",
    "            )\n",
    "            trainloader = DataLoader(\n",
    "                trainset, batch_size=batch_size,\n",
    "                shuffle=True, num_workers=2, pin_memory=True\n",
    "            )\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.1, \n",
    "                                momentum=0.9, weight_decay=5e-4)\n",
    "            \n",
    "            # Warmup epoch (not timed)\n",
    "            net.train()\n",
    "            for inputs, targets in trainloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Measured epoch\n",
    "            torch.cuda.synchronize()  # Ensure all GPU operations complete\n",
    "            start_time = time.time()\n",
    "            \n",
    "            net.train()\n",
    "            for inputs, targets in trainloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            torch.cuda.synchronize()  # Ensure all GPU operations complete\n",
    "            end_time = time.time()\n",
    "            \n",
    "            epoch_time = end_time - start_time\n",
    "            results[batch_size] = epoch_time\n",
    "            print(f\"Batch size {batch_size}: {epoch_time:.2f}s\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"Batch size {batch_size}: OOM - stopping\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193f2a3-58db-452a-a546-4e4bf23d5822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS=32, GPUs=1: 19.82s, Speedup=1.00x\n",
      "BS=32, GPUs=2: 57.82s, Speedup=0.34x\n",
      "BS=32, GPUs=4: 75.65s, Speedup=0.26x\n",
      "BS=128, GPUs=1: 17.75s, Speedup=1.00x\n",
      "BS=128, GPUs=2: 17.41s, Speedup=1.02x\n",
      "BS=128, GPUs=4: 19.26s, Speedup=0.92x\n",
      "BS=512, GPUs=1: 19.76s, Speedup=1.00x\n",
      "BS=512, GPUs=2: 10.57s, Speedup=1.87x\n",
      "BS=512, GPUs=4: 9.38s, Speedup=2.11x\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Using the function for multiple GPU-s\n",
    "def measure_multi_gpu_training(batch_size, num_gpus):\n",
    "    \"\"\"\n",
    "    Measure total training time (including data loading) on multiple GPUs\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create model\n",
    "    net = resnet.ResNet18()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    # Apply DataParallel for multi-GPU\n",
    "    if num_gpus > 1:\n",
    "        net = nn.DataParallel(net, device_ids=list(range(num_gpus)))\n",
    "    net = net.to(device)\n",
    "    \n",
    "    # Create dataloader\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True,\n",
    "        transform=transform_train\n",
    "    )\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, \n",
    "                        momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    # Warmup epoch\n",
    "    net.train()\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Measured epoch (include everything)\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    net.train()\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Run experiments\n",
    "batch_sizes = [32, 128, 512]  # Adjust based on Part 1 results\n",
    "gpu_configs = [1, 2, 4]\n",
    "\n",
    "results_table = {}\n",
    "for bs in batch_sizes:\n",
    "    results_table[bs] = {}\n",
    "    for num_gpus in gpu_configs:\n",
    "        time_taken = measure_multi_gpu_training(bs, num_gpus)\n",
    "        results_table[bs][num_gpus] = {\n",
    "            'time': time_taken,\n",
    "            'speedup': results_table[bs][1]['time'] / time_taken if num_gpus > 1 else 1.0\n",
    "        }\n",
    "        print(f\"BS={bs}, GPUs={num_gpus}: {time_taken:.2f}s, Speedup={results_table[bs][num_gpus]['speedup']:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "086fd045-0ae1-4917-955b-be6e2a22aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3 Communication\n",
    "def calculate_compute_communication_breakdown(results_table, single_gpu_results):\n",
    "    \"\"\"\n",
    "    Calculate compute and communication time for multi-GPU setups\n",
    "    \n",
    "    Compute time ≈ single GPU time (same batch size, same computation)\n",
    "    Communication time = Total multi-GPU time - Compute time\n",
    "    \"\"\"\n",
    "    breakdown = {}\n",
    "    \n",
    "    for batch_size in results_table.keys():\n",
    "        breakdown[batch_size] = {}\n",
    "        \n",
    "        # Single GPU compute time (no communication)\n",
    "        compute_time_base = single_gpu_results[batch_size]\n",
    "        \n",
    "        for num_gpus in [2, 4]:\n",
    "            total_time = results_table[batch_size][num_gpus]['time']\n",
    "            \n",
    "            # Compute time: same as single GPU for same batch size per GPU\n",
    "            compute_time = compute_time_base\n",
    "            \n",
    "            # Communication time: includes gradient allreduce\n",
    "            communication_time = total_time - compute_time\n",
    "            \n",
    "            breakdown[batch_size][num_gpus] = {\n",
    "                'compute': compute_time,\n",
    "                'communication': communication_time,\n",
    "                'total': total_time\n",
    "            }\n",
    "    \n",
    "    return breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30e37e8b-45ae-4402-8a33-f60bb3957191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 Bandwith utilization\n",
    "def calculate_bandwidth_utilization(model, breakdown_table, num_gpus_list=[2, 4]):\n",
    "    \"\"\"\n",
    "    Calculate bandwidth utilization for allreduce operations\n",
    "    \n",
    "    Formulas:\n",
    "    - Ring AllReduce time: T_comm = 2 * (N-1) / N * S / B\n",
    "      where N = number of GPUs, S = model size in bytes, B = bandwidth\n",
    "    - Bandwidth utilization: B_effective = 2 * (N-1) / N * S / T_measured\n",
    "    \"\"\"\n",
    "    # Calculate model size (number of parameters * 4 bytes for float32)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    model_size_bytes = total_params * 4  # float32\n",
    "    model_size_gb = model_size_bytes / (1024**3)\n",
    "    \n",
    "    print(f\"Model size: {total_params:,} parameters = {model_size_gb:.4f} GB\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for batch_size, gpu_data in breakdown_table.items():\n",
    "        results[batch_size] = {}\n",
    "        \n",
    "        for num_gpus in num_gpus_list:\n",
    "            if num_gpus in gpu_data:\n",
    "                # Measured communication time\n",
    "                T_measured = gpu_data[num_gpus]['communication']\n",
    "                \n",
    "                # Ring allreduce coefficient\n",
    "                ring_factor = 2 * (num_gpus - 1) / num_gpus\n",
    "                \n",
    "                # Effective bandwidth (GB/s)\n",
    "                B_effective = ring_factor * model_size_gb / T_measured\n",
    "                \n",
    "                # Theoretical time formula: T = 2*(N-1)/N * S/B\n",
    "                # (assuming peak bandwidth, e.g., NVLink 300 GB/s for A100)\n",
    "                B_peak = 300  # GB/s for NVLink on A100\n",
    "                T_theoretical = ring_factor * model_size_gb / B_peak\n",
    "                \n",
    "                # Bandwidth utilization percentage\n",
    "                utilization = (B_effective / B_peak) * 100\n",
    "                \n",
    "                results[batch_size][num_gpus] = {\n",
    "                    'T_measured': T_measured,\n",
    "                    'T_theoretical': T_theoretical,\n",
    "                    'B_effective': B_effective,\n",
    "                    'B_peak': B_peak,\n",
    "                    'utilization': utilization\n",
    "                }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3918d4a4-eac6-49e3-9cea-3bbd51c56865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch: 0/50\n",
      "============================================================\n",
      "Epoch: 0 | Batch: 0/391 | Loss: 0.277 | Acc: 90.62% (116/128)\n",
      "Epoch: 0 | Batch: 100/391 | Loss: 0.324 | Acc: 89.02% (11509/12928)\n",
      "Epoch: 0 | Batch: 200/391 | Loss: 0.325 | Acc: 88.92% (22878/25728)\n",
      "Epoch: 0 | Batch: 300/391 | Loss: 0.327 | Acc: 88.89% (34247/38528)\n",
      "Test Loss: 0.515 | Test Acc: 82.96% (8296/10000)\n",
      "Saving checkpoint...\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 82.96%\n",
      "\n",
      "============================================================\n",
      "Epoch: 1/50\n",
      "============================================================\n",
      "Epoch: 1 | Batch: 0/391 | Loss: 0.289 | Acc: 89.06% (114/128)\n",
      "Epoch: 1 | Batch: 100/391 | Loss: 0.324 | Acc: 89.07% (11515/12928)\n",
      "Epoch: 1 | Batch: 200/391 | Loss: 0.325 | Acc: 88.93% (22879/25728)\n",
      "Epoch: 1 | Batch: 300/391 | Loss: 0.334 | Acc: 88.68% (34165/38528)\n",
      "Test Loss: 0.503 | Test Acc: 83.80% (8380/10000)\n",
      "Saving checkpoint...\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 83.80%\n",
      "\n",
      "============================================================\n",
      "Epoch: 2/50\n",
      "============================================================\n",
      "Epoch: 2 | Batch: 0/391 | Loss: 0.356 | Acc: 85.94% (110/128)\n",
      "Epoch: 2 | Batch: 100/391 | Loss: 0.319 | Acc: 89.38% (11555/12928)\n",
      "Epoch: 2 | Batch: 200/391 | Loss: 0.323 | Acc: 89.16% (22938/25728)\n",
      "Epoch: 2 | Batch: 300/391 | Loss: 0.329 | Acc: 88.89% (34246/38528)\n",
      "Test Loss: 0.470 | Test Acc: 84.92% (8492/10000)\n",
      "Saving checkpoint...\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 84.92%\n",
      "\n",
      "============================================================\n",
      "Epoch: 3/50\n",
      "============================================================\n",
      "Epoch: 3 | Batch: 0/391 | Loss: 0.394 | Acc: 89.06% (114/128)\n",
      "Epoch: 3 | Batch: 100/391 | Loss: 0.315 | Acc: 89.23% (11536/12928)\n",
      "Epoch: 3 | Batch: 200/391 | Loss: 0.320 | Acc: 89.10% (22924/25728)\n",
      "Epoch: 3 | Batch: 300/391 | Loss: 0.327 | Acc: 88.90% (34250/38528)\n",
      "Test Loss: 0.507 | Test Acc: 83.53% (8353/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 84.92%\n",
      "\n",
      "============================================================\n",
      "Epoch: 4/50\n",
      "============================================================\n",
      "Epoch: 4 | Batch: 0/391 | Loss: 0.284 | Acc: 90.62% (116/128)\n",
      "Epoch: 4 | Batch: 100/391 | Loss: 0.342 | Acc: 88.33% (11419/12928)\n",
      "Epoch: 4 | Batch: 200/391 | Loss: 0.330 | Acc: 88.94% (22882/25728)\n",
      "Epoch: 4 | Batch: 300/391 | Loss: 0.339 | Acc: 88.60% (34135/38528)\n",
      "Test Loss: 0.381 | Test Acc: 87.53% (8753/10000)\n",
      "Saving checkpoint...\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 5/50\n",
      "============================================================\n",
      "Epoch: 5 | Batch: 0/391 | Loss: 0.199 | Acc: 95.31% (122/128)\n",
      "Epoch: 5 | Batch: 100/391 | Loss: 0.316 | Acc: 89.60% (11583/12928)\n",
      "Epoch: 5 | Batch: 200/391 | Loss: 0.323 | Acc: 89.25% (22963/25728)\n",
      "Epoch: 5 | Batch: 300/391 | Loss: 0.334 | Acc: 88.82% (34219/38528)\n",
      "Test Loss: 0.408 | Test Acc: 86.32% (8632/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 6/50\n",
      "============================================================\n",
      "Epoch: 6 | Batch: 0/391 | Loss: 0.241 | Acc: 92.19% (118/128)\n",
      "Epoch: 6 | Batch: 100/391 | Loss: 0.300 | Acc: 89.68% (11594/12928)\n",
      "Epoch: 6 | Batch: 200/391 | Loss: 0.329 | Acc: 88.80% (22847/25728)\n",
      "Epoch: 6 | Batch: 300/391 | Loss: 0.328 | Acc: 88.89% (34248/38528)\n",
      "Test Loss: 0.476 | Test Acc: 84.49% (8449/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 7/50\n",
      "============================================================\n",
      "Epoch: 7 | Batch: 0/391 | Loss: 0.344 | Acc: 87.50% (112/128)\n",
      "Epoch: 7 | Batch: 100/391 | Loss: 0.322 | Acc: 89.01% (11507/12928)\n",
      "Epoch: 7 | Batch: 200/391 | Loss: 0.333 | Acc: 88.67% (22814/25728)\n",
      "Epoch: 7 | Batch: 300/391 | Loss: 0.333 | Acc: 88.67% (34163/38528)\n",
      "Test Loss: 0.532 | Test Acc: 83.24% (8324/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 8/50\n",
      "============================================================\n",
      "Epoch: 8 | Batch: 0/391 | Loss: 0.360 | Acc: 86.72% (111/128)\n",
      "Epoch: 8 | Batch: 100/391 | Loss: 0.324 | Acc: 89.16% (11527/12928)\n",
      "Epoch: 8 | Batch: 200/391 | Loss: 0.333 | Acc: 88.77% (22839/25728)\n",
      "Epoch: 8 | Batch: 300/391 | Loss: 0.332 | Acc: 88.75% (34195/38528)\n",
      "Test Loss: 0.471 | Test Acc: 84.73% (8473/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 9/50\n",
      "============================================================\n",
      "Epoch: 9 | Batch: 0/391 | Loss: 0.347 | Acc: 89.06% (114/128)\n",
      "Epoch: 9 | Batch: 100/391 | Loss: 0.310 | Acc: 89.49% (11569/12928)\n",
      "Epoch: 9 | Batch: 200/391 | Loss: 0.325 | Acc: 88.91% (22874/25728)\n",
      "Epoch: 9 | Batch: 300/391 | Loss: 0.328 | Acc: 88.90% (34252/38528)\n",
      "Test Loss: 0.530 | Test Acc: 82.69% (8269/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 10/50\n",
      "============================================================\n",
      "Epoch: 10 | Batch: 0/391 | Loss: 0.205 | Acc: 92.97% (119/128)\n",
      "Epoch: 10 | Batch: 100/391 | Loss: 0.316 | Acc: 89.43% (11562/12928)\n",
      "Epoch: 10 | Batch: 200/391 | Loss: 0.321 | Acc: 89.25% (22961/25728)\n",
      "Epoch: 10 | Batch: 300/391 | Loss: 0.326 | Acc: 89.07% (34317/38528)\n",
      "Test Loss: 0.469 | Test Acc: 84.73% (8473/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 11/50\n",
      "============================================================\n",
      "Epoch: 11 | Batch: 0/391 | Loss: 0.383 | Acc: 85.94% (110/128)\n",
      "Epoch: 11 | Batch: 100/391 | Loss: 0.317 | Acc: 89.09% (11517/12928)\n",
      "Epoch: 11 | Batch: 200/391 | Loss: 0.323 | Acc: 89.03% (22906/25728)\n",
      "Epoch: 11 | Batch: 300/391 | Loss: 0.330 | Acc: 88.74% (34190/38528)\n",
      "Test Loss: 0.610 | Test Acc: 80.21% (8021/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 12/50\n",
      "============================================================\n",
      "Epoch: 12 | Batch: 0/391 | Loss: 0.454 | Acc: 85.94% (110/128)\n",
      "Epoch: 12 | Batch: 100/391 | Loss: 0.326 | Acc: 88.97% (11502/12928)\n",
      "Epoch: 12 | Batch: 200/391 | Loss: 0.332 | Acc: 88.79% (22845/25728)\n",
      "Epoch: 12 | Batch: 300/391 | Loss: 0.335 | Acc: 88.71% (34179/38528)\n",
      "Test Loss: 0.398 | Test Acc: 86.66% (8666/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 13/50\n",
      "============================================================\n",
      "Epoch: 13 | Batch: 0/391 | Loss: 0.246 | Acc: 90.62% (116/128)\n",
      "Epoch: 13 | Batch: 100/391 | Loss: 0.326 | Acc: 88.81% (11482/12928)\n",
      "Epoch: 13 | Batch: 200/391 | Loss: 0.329 | Acc: 88.74% (22830/25728)\n",
      "Epoch: 13 | Batch: 300/391 | Loss: 0.332 | Acc: 88.73% (34184/38528)\n",
      "Test Loss: 0.409 | Test Acc: 86.25% (8625/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 14/50\n",
      "============================================================\n",
      "Epoch: 14 | Batch: 0/391 | Loss: 0.313 | Acc: 90.62% (116/128)\n",
      "Epoch: 14 | Batch: 100/391 | Loss: 0.322 | Acc: 89.12% (11522/12928)\n",
      "Epoch: 14 | Batch: 200/391 | Loss: 0.323 | Acc: 89.08% (22919/25728)\n",
      "Epoch: 14 | Batch: 300/391 | Loss: 0.333 | Acc: 88.78% (34207/38528)\n",
      "Test Loss: 0.469 | Test Acc: 85.03% (8503/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 15/50\n",
      "============================================================\n",
      "Epoch: 15 | Batch: 0/391 | Loss: 0.404 | Acc: 89.06% (114/128)\n",
      "Epoch: 15 | Batch: 100/391 | Loss: 0.302 | Acc: 89.67% (11592/12928)\n",
      "Epoch: 15 | Batch: 200/391 | Loss: 0.314 | Acc: 89.31% (22978/25728)\n",
      "Epoch: 15 | Batch: 300/391 | Loss: 0.321 | Acc: 89.14% (34344/38528)\n",
      "Test Loss: 0.531 | Test Acc: 82.88% (8288/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 16/50\n",
      "============================================================\n",
      "Epoch: 16 | Batch: 0/391 | Loss: 0.283 | Acc: 91.41% (117/128)\n",
      "Epoch: 16 | Batch: 100/391 | Loss: 0.311 | Acc: 89.33% (11548/12928)\n",
      "Epoch: 16 | Batch: 200/391 | Loss: 0.321 | Acc: 89.12% (22928/25728)\n",
      "Epoch: 16 | Batch: 300/391 | Loss: 0.327 | Acc: 88.93% (34262/38528)\n",
      "Test Loss: 0.412 | Test Acc: 86.29% (8629/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 17/50\n",
      "============================================================\n",
      "Epoch: 17 | Batch: 0/391 | Loss: 0.216 | Acc: 93.75% (120/128)\n",
      "Epoch: 17 | Batch: 100/391 | Loss: 0.316 | Acc: 89.39% (11556/12928)\n",
      "Epoch: 17 | Batch: 200/391 | Loss: 0.320 | Acc: 89.35% (22989/25728)\n",
      "Epoch: 17 | Batch: 300/391 | Loss: 0.333 | Acc: 88.88% (34245/38528)\n",
      "Test Loss: 0.519 | Test Acc: 82.92% (8292/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 18/50\n",
      "============================================================\n",
      "Epoch: 18 | Batch: 0/391 | Loss: 0.368 | Acc: 87.50% (112/128)\n",
      "Epoch: 18 | Batch: 100/391 | Loss: 0.319 | Acc: 88.90% (11493/12928)\n",
      "Epoch: 18 | Batch: 200/391 | Loss: 0.325 | Acc: 88.83% (22855/25728)\n",
      "Epoch: 18 | Batch: 300/391 | Loss: 0.330 | Acc: 88.72% (34181/38528)\n",
      "Test Loss: 0.485 | Test Acc: 84.33% (8433/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 19/50\n",
      "============================================================\n",
      "Epoch: 19 | Batch: 0/391 | Loss: 0.150 | Acc: 96.09% (123/128)\n",
      "Epoch: 19 | Batch: 100/391 | Loss: 0.318 | Acc: 89.22% (11534/12928)\n",
      "Epoch: 19 | Batch: 200/391 | Loss: 0.326 | Acc: 88.99% (22895/25728)\n",
      "Epoch: 19 | Batch: 300/391 | Loss: 0.330 | Acc: 88.79% (34209/38528)\n",
      "Test Loss: 0.447 | Test Acc: 85.52% (8552/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 20/50\n",
      "============================================================\n",
      "Epoch: 20 | Batch: 0/391 | Loss: 0.397 | Acc: 85.16% (109/128)\n",
      "Epoch: 20 | Batch: 100/391 | Loss: 0.318 | Acc: 88.99% (11504/12928)\n",
      "Epoch: 20 | Batch: 200/391 | Loss: 0.316 | Acc: 89.08% (22919/25728)\n",
      "Epoch: 20 | Batch: 300/391 | Loss: 0.327 | Acc: 88.82% (34222/38528)\n",
      "Test Loss: 0.419 | Test Acc: 86.04% (8604/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 21/50\n",
      "============================================================\n",
      "Epoch: 21 | Batch: 0/391 | Loss: 0.394 | Acc: 86.72% (111/128)\n",
      "Epoch: 21 | Batch: 100/391 | Loss: 0.311 | Acc: 89.25% (11538/12928)\n",
      "Epoch: 21 | Batch: 200/391 | Loss: 0.324 | Acc: 88.96% (22887/25728)\n",
      "Epoch: 21 | Batch: 300/391 | Loss: 0.325 | Acc: 89.05% (34311/38528)\n",
      "Test Loss: 0.475 | Test Acc: 84.25% (8425/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 22/50\n",
      "============================================================\n",
      "Epoch: 22 | Batch: 0/391 | Loss: 0.319 | Acc: 91.41% (117/128)\n",
      "Epoch: 22 | Batch: 100/391 | Loss: 0.324 | Acc: 88.95% (11500/12928)\n",
      "Epoch: 22 | Batch: 200/391 | Loss: 0.326 | Acc: 89.05% (22911/25728)\n",
      "Epoch: 22 | Batch: 300/391 | Loss: 0.331 | Acc: 88.85% (34233/38528)\n",
      "Test Loss: 0.570 | Test Acc: 81.08% (8108/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.53%\n",
      "\n",
      "============================================================\n",
      "Epoch: 23/50\n",
      "============================================================\n",
      "Epoch: 23 | Batch: 0/391 | Loss: 0.309 | Acc: 86.72% (111/128)\n",
      "Epoch: 23 | Batch: 100/391 | Loss: 0.326 | Acc: 89.02% (11508/12928)\n",
      "Epoch: 23 | Batch: 200/391 | Loss: 0.329 | Acc: 88.88% (22867/25728)\n",
      "Epoch: 23 | Batch: 300/391 | Loss: 0.336 | Acc: 88.68% (34168/38528)\n",
      "Test Loss: 0.366 | Test Acc: 87.55% (8755/10000)\n",
      "Saving checkpoint...\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 24/50\n",
      "============================================================\n",
      "Epoch: 24 | Batch: 0/391 | Loss: 0.280 | Acc: 87.50% (112/128)\n",
      "Epoch: 24 | Batch: 100/391 | Loss: 0.310 | Acc: 89.67% (11593/12928)\n",
      "Epoch: 24 | Batch: 200/391 | Loss: 0.320 | Acc: 89.27% (22968/25728)\n",
      "Epoch: 24 | Batch: 300/391 | Loss: 0.323 | Acc: 89.18% (34359/38528)\n",
      "Test Loss: 0.414 | Test Acc: 86.16% (8616/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 25/50\n",
      "============================================================\n",
      "Epoch: 25 | Batch: 0/391 | Loss: 0.278 | Acc: 87.50% (112/128)\n",
      "Epoch: 25 | Batch: 100/391 | Loss: 0.316 | Acc: 89.19% (11530/12928)\n",
      "Epoch: 25 | Batch: 200/391 | Loss: 0.323 | Acc: 88.98% (22893/25728)\n",
      "Epoch: 25 | Batch: 300/391 | Loss: 0.328 | Acc: 88.90% (34251/38528)\n",
      "Test Loss: 0.482 | Test Acc: 84.31% (8431/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 26/50\n",
      "============================================================\n",
      "Epoch: 26 | Batch: 0/391 | Loss: 0.365 | Acc: 84.38% (108/128)\n",
      "Epoch: 26 | Batch: 100/391 | Loss: 0.328 | Acc: 89.05% (11513/12928)\n",
      "Epoch: 26 | Batch: 200/391 | Loss: 0.334 | Acc: 88.84% (22857/25728)\n",
      "Epoch: 26 | Batch: 300/391 | Loss: 0.331 | Acc: 88.94% (34268/38528)\n",
      "Test Loss: 0.731 | Test Acc: 77.98% (7798/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 27/50\n",
      "============================================================\n",
      "Epoch: 27 | Batch: 0/391 | Loss: 0.142 | Acc: 95.31% (122/128)\n",
      "Epoch: 27 | Batch: 100/391 | Loss: 0.301 | Acc: 89.79% (11608/12928)\n",
      "Epoch: 27 | Batch: 200/391 | Loss: 0.324 | Acc: 89.00% (22899/25728)\n",
      "Epoch: 27 | Batch: 300/391 | Loss: 0.335 | Acc: 88.60% (34137/38528)\n",
      "Test Loss: 0.416 | Test Acc: 86.38% (8638/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 28/50\n",
      "============================================================\n",
      "Epoch: 28 | Batch: 0/391 | Loss: 0.339 | Acc: 88.28% (113/128)\n",
      "Epoch: 28 | Batch: 100/391 | Loss: 0.312 | Acc: 89.32% (11547/12928)\n",
      "Epoch: 28 | Batch: 200/391 | Loss: 0.329 | Acc: 88.87% (22864/25728)\n",
      "Epoch: 28 | Batch: 300/391 | Loss: 0.329 | Acc: 88.89% (34248/38528)\n",
      "Test Loss: 0.491 | Test Acc: 83.83% (8383/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 29/50\n",
      "============================================================\n",
      "Epoch: 29 | Batch: 0/391 | Loss: 0.220 | Acc: 92.97% (119/128)\n",
      "Epoch: 29 | Batch: 100/391 | Loss: 0.321 | Acc: 89.36% (11553/12928)\n",
      "Epoch: 29 | Batch: 200/391 | Loss: 0.321 | Acc: 89.34% (22985/25728)\n",
      "Epoch: 29 | Batch: 300/391 | Loss: 0.325 | Acc: 89.15% (34346/38528)\n",
      "Test Loss: 0.438 | Test Acc: 85.50% (8550/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 30/50\n",
      "============================================================\n",
      "Epoch: 30 | Batch: 0/391 | Loss: 0.275 | Acc: 90.62% (116/128)\n",
      "Epoch: 30 | Batch: 100/391 | Loss: 0.315 | Acc: 89.55% (11577/12928)\n",
      "Epoch: 30 | Batch: 200/391 | Loss: 0.326 | Acc: 89.04% (22909/25728)\n",
      "Epoch: 30 | Batch: 300/391 | Loss: 0.332 | Acc: 88.80% (34212/38528)\n",
      "Test Loss: 0.416 | Test Acc: 86.52% (8652/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 31/50\n",
      "============================================================\n",
      "Epoch: 31 | Batch: 0/391 | Loss: 0.314 | Acc: 89.06% (114/128)\n",
      "Epoch: 31 | Batch: 100/391 | Loss: 0.311 | Acc: 89.44% (11563/12928)\n",
      "Epoch: 31 | Batch: 200/391 | Loss: 0.316 | Acc: 89.35% (22987/25728)\n",
      "Epoch: 31 | Batch: 300/391 | Loss: 0.322 | Acc: 89.17% (34357/38528)\n",
      "Test Loss: 0.441 | Test Acc: 85.57% (8557/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 32/50\n",
      "============================================================\n",
      "Epoch: 32 | Batch: 0/391 | Loss: 0.343 | Acc: 86.72% (111/128)\n",
      "Epoch: 32 | Batch: 100/391 | Loss: 0.303 | Acc: 89.91% (11623/12928)\n",
      "Epoch: 32 | Batch: 200/391 | Loss: 0.313 | Acc: 89.45% (23013/25728)\n",
      "Epoch: 32 | Batch: 300/391 | Loss: 0.322 | Acc: 89.16% (34350/38528)\n",
      "Test Loss: 0.499 | Test Acc: 83.61% (8361/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 33/50\n",
      "============================================================\n",
      "Epoch: 33 | Batch: 0/391 | Loss: 0.222 | Acc: 92.97% (119/128)\n",
      "Epoch: 33 | Batch: 100/391 | Loss: 0.317 | Acc: 89.29% (11544/12928)\n",
      "Epoch: 33 | Batch: 200/391 | Loss: 0.324 | Acc: 89.03% (22906/25728)\n",
      "Epoch: 33 | Batch: 300/391 | Loss: 0.326 | Acc: 88.98% (34282/38528)\n",
      "Test Loss: 0.402 | Test Acc: 86.33% (8633/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 34/50\n",
      "============================================================\n",
      "Epoch: 34 | Batch: 0/391 | Loss: 0.306 | Acc: 89.84% (115/128)\n",
      "Epoch: 34 | Batch: 100/391 | Loss: 0.309 | Acc: 89.58% (11581/12928)\n",
      "Epoch: 34 | Batch: 200/391 | Loss: 0.320 | Acc: 89.21% (22952/25728)\n",
      "Epoch: 34 | Batch: 300/391 | Loss: 0.324 | Acc: 89.08% (34322/38528)\n",
      "Test Loss: 0.387 | Test Acc: 87.45% (8745/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 35/50\n",
      "============================================================\n",
      "Epoch: 35 | Batch: 0/391 | Loss: 0.252 | Acc: 92.97% (119/128)\n",
      "Epoch: 35 | Batch: 100/391 | Loss: 0.323 | Acc: 89.13% (11523/12928)\n",
      "Epoch: 35 | Batch: 200/391 | Loss: 0.327 | Acc: 88.82% (22851/25728)\n",
      "Epoch: 35 | Batch: 300/391 | Loss: 0.332 | Acc: 88.67% (34162/38528)\n",
      "Test Loss: 0.468 | Test Acc: 84.80% (8480/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 36/50\n",
      "============================================================\n",
      "Epoch: 36 | Batch: 0/391 | Loss: 0.198 | Acc: 94.53% (121/128)\n",
      "Epoch: 36 | Batch: 100/391 | Loss: 0.316 | Acc: 88.90% (11493/12928)\n",
      "Epoch: 36 | Batch: 200/391 | Loss: 0.329 | Acc: 88.69% (22818/25728)\n",
      "Epoch: 36 | Batch: 300/391 | Loss: 0.332 | Acc: 88.64% (34150/38528)\n",
      "Test Loss: 0.463 | Test Acc: 84.63% (8463/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 37/50\n",
      "============================================================\n",
      "Epoch: 37 | Batch: 0/391 | Loss: 0.259 | Acc: 89.84% (115/128)\n",
      "Epoch: 37 | Batch: 100/391 | Loss: 0.316 | Acc: 89.49% (11569/12928)\n",
      "Epoch: 37 | Batch: 200/391 | Loss: 0.322 | Acc: 89.02% (22903/25728)\n",
      "Epoch: 37 | Batch: 300/391 | Loss: 0.325 | Acc: 88.92% (34259/38528)\n",
      "Test Loss: 0.458 | Test Acc: 84.99% (8499/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 38/50\n",
      "============================================================\n",
      "Epoch: 38 | Batch: 0/391 | Loss: 0.188 | Acc: 92.19% (118/128)\n",
      "Epoch: 38 | Batch: 100/391 | Loss: 0.311 | Acc: 89.36% (11553/12928)\n",
      "Epoch: 38 | Batch: 200/391 | Loss: 0.316 | Acc: 89.24% (22959/25728)\n",
      "Epoch: 38 | Batch: 300/391 | Loss: 0.322 | Acc: 88.97% (34280/38528)\n",
      "Test Loss: 0.432 | Test Acc: 85.46% (8546/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 39/50\n",
      "============================================================\n",
      "Epoch: 39 | Batch: 0/391 | Loss: 0.252 | Acc: 91.41% (117/128)\n",
      "Epoch: 39 | Batch: 100/391 | Loss: 0.326 | Acc: 88.74% (11472/12928)\n",
      "Epoch: 39 | Batch: 200/391 | Loss: 0.334 | Acc: 88.45% (22757/25728)\n",
      "Epoch: 39 | Batch: 300/391 | Loss: 0.332 | Acc: 88.67% (34162/38528)\n",
      "Test Loss: 0.437 | Test Acc: 85.58% (8558/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 40/50\n",
      "============================================================\n",
      "Epoch: 40 | Batch: 0/391 | Loss: 0.333 | Acc: 86.72% (111/128)\n",
      "Epoch: 40 | Batch: 100/391 | Loss: 0.310 | Acc: 89.70% (11596/12928)\n",
      "Epoch: 40 | Batch: 200/391 | Loss: 0.322 | Acc: 89.32% (22980/25728)\n",
      "Epoch: 40 | Batch: 300/391 | Loss: 0.327 | Acc: 89.05% (34310/38528)\n",
      "Test Loss: 0.498 | Test Acc: 84.19% (8419/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 41/50\n",
      "============================================================\n",
      "Epoch: 41 | Batch: 0/391 | Loss: 0.262 | Acc: 92.19% (118/128)\n",
      "Epoch: 41 | Batch: 100/391 | Loss: 0.330 | Acc: 88.84% (11485/12928)\n",
      "Epoch: 41 | Batch: 200/391 | Loss: 0.329 | Acc: 88.72% (22825/25728)\n",
      "Epoch: 41 | Batch: 300/391 | Loss: 0.334 | Acc: 88.67% (34162/38528)\n",
      "Test Loss: 0.494 | Test Acc: 84.56% (8456/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 42/50\n",
      "============================================================\n",
      "Epoch: 42 | Batch: 0/391 | Loss: 0.291 | Acc: 91.41% (117/128)\n",
      "Epoch: 42 | Batch: 100/391 | Loss: 0.316 | Acc: 89.10% (11519/12928)\n",
      "Epoch: 42 | Batch: 200/391 | Loss: 0.323 | Acc: 88.88% (22867/25728)\n",
      "Epoch: 42 | Batch: 300/391 | Loss: 0.327 | Acc: 88.78% (34207/38528)\n",
      "Test Loss: 0.494 | Test Acc: 83.56% (8356/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 43/50\n",
      "============================================================\n",
      "Epoch: 43 | Batch: 0/391 | Loss: 0.416 | Acc: 85.16% (109/128)\n",
      "Epoch: 43 | Batch: 100/391 | Loss: 0.317 | Acc: 89.17% (11528/12928)\n",
      "Epoch: 43 | Batch: 200/391 | Loss: 0.317 | Acc: 89.19% (22947/25728)\n",
      "Epoch: 43 | Batch: 300/391 | Loss: 0.324 | Acc: 89.00% (34290/38528)\n",
      "Test Loss: 0.418 | Test Acc: 85.85% (8585/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 44/50\n",
      "============================================================\n",
      "Epoch: 44 | Batch: 0/391 | Loss: 0.276 | Acc: 89.84% (115/128)\n",
      "Epoch: 44 | Batch: 100/391 | Loss: 0.311 | Acc: 89.49% (11569/12928)\n",
      "Epoch: 44 | Batch: 200/391 | Loss: 0.310 | Acc: 89.47% (23018/25728)\n",
      "Epoch: 44 | Batch: 300/391 | Loss: 0.319 | Acc: 89.19% (34363/38528)\n",
      "Test Loss: 0.458 | Test Acc: 84.84% (8484/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 45/50\n",
      "============================================================\n",
      "Epoch: 45 | Batch: 0/391 | Loss: 0.284 | Acc: 89.06% (114/128)\n",
      "Epoch: 45 | Batch: 100/391 | Loss: 0.302 | Acc: 89.58% (11581/12928)\n",
      "Epoch: 45 | Batch: 200/391 | Loss: 0.315 | Acc: 89.34% (22985/25728)\n",
      "Epoch: 45 | Batch: 300/391 | Loss: 0.323 | Acc: 89.10% (34328/38528)\n",
      "Test Loss: 0.403 | Test Acc: 86.43% (8643/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 46/50\n",
      "============================================================\n",
      "Epoch: 46 | Batch: 0/391 | Loss: 0.320 | Acc: 90.62% (116/128)\n",
      "Epoch: 46 | Batch: 100/391 | Loss: 0.326 | Acc: 89.05% (11513/12928)\n",
      "Epoch: 46 | Batch: 200/391 | Loss: 0.319 | Acc: 89.13% (22932/25728)\n",
      "Epoch: 46 | Batch: 300/391 | Loss: 0.322 | Acc: 89.04% (34305/38528)\n",
      "Test Loss: 0.569 | Test Acc: 81.37% (8137/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 47/50\n",
      "============================================================\n",
      "Epoch: 47 | Batch: 0/391 | Loss: 0.265 | Acc: 88.28% (113/128)\n",
      "Epoch: 47 | Batch: 100/391 | Loss: 0.311 | Acc: 89.34% (11550/12928)\n",
      "Epoch: 47 | Batch: 200/391 | Loss: 0.312 | Acc: 89.43% (23008/25728)\n",
      "Epoch: 47 | Batch: 300/391 | Loss: 0.316 | Acc: 89.34% (34422/38528)\n",
      "Test Loss: 0.378 | Test Acc: 87.21% (8721/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 48/50\n",
      "============================================================\n",
      "Epoch: 48 | Batch: 0/391 | Loss: 0.252 | Acc: 91.41% (117/128)\n",
      "Epoch: 48 | Batch: 100/391 | Loss: 0.318 | Acc: 89.09% (11517/12928)\n",
      "Epoch: 48 | Batch: 200/391 | Loss: 0.319 | Acc: 89.14% (22933/25728)\n",
      "Epoch: 48 | Batch: 300/391 | Loss: 0.324 | Acc: 89.05% (34309/38528)\n",
      "Test Loss: 0.525 | Test Acc: 83.06% (8306/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "============================================================\n",
      "Epoch: 49/50\n",
      "============================================================\n",
      "Epoch: 49 | Batch: 0/391 | Loss: 0.428 | Acc: 84.38% (108/128)\n",
      "Epoch: 49 | Batch: 100/391 | Loss: 0.315 | Acc: 89.17% (11528/12928)\n",
      "Epoch: 49 | Batch: 200/391 | Loss: 0.315 | Acc: 89.20% (22950/25728)\n",
      "Epoch: 49 | Batch: 300/391 | Loss: 0.322 | Acc: 89.03% (34300/38528)\n",
      "Test Loss: 0.477 | Test Acc: 83.67% (8367/10000)\n",
      "Current LR: 0.100000\n",
      "Best Test Acc: 87.55%\n",
      "\n",
      "Training Complete! Best test accuracy: 87.55%\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "num_epochs = 50\n",
    "best_acc = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "# Create checkpoint directory\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Epoch: {epoch}/{num_epochs}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(net, trainloader, criterion, optimizer, device, epoch)\n",
    "    test_loss, test_acc = test(net, testloader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    # Save checkpoint if best accuracy\n",
    "    if test_acc > best_acc:\n",
    "        print('Saving checkpoint...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, './checkpoint/resnet18_cifar10_best.pth')\n",
    "        best_acc = test_acc\n",
    "    \n",
    "    print(f'Current LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    print(f'Best Test Acc: {best_acc:.2f}%')\n",
    "\n",
    "print(f'\\nTraining Complete! Best test accuracy: {best_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5a561-f692-46fb-9381-1eaedde08ee3",
   "metadata": {},
   "source": [
    "After training 50 epochs we got the best accuracy as 87.55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e8ffac8-d719-4d0f-8627-b04bf2d5233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Part 1: Single GPU with varying batch sizes ===\n",
      "Batch size 32: 20.15s\n",
      "Batch size 128: 17.63s\n",
      "Batch size 512: 19.66s\n",
      "\n",
      "=== Part 2: Multi-GPU training and speedup ===\n",
      "BS=32, GPUs=1: 20.20s, Speedup=1.00x\n",
      "BS=32, GPUs=2: 58.31s, Speedup=0.35x\n",
      "BS=32, GPUs=4: 76.15s, Speedup=0.27x\n",
      "BS=128, GPUs=1: 17.74s, Speedup=1.00x\n",
      "BS=128, GPUs=2: 17.48s, Speedup=1.01x\n",
      "BS=128, GPUs=4: 19.16s, Speedup=0.93x\n",
      "BS=512, GPUs=1: 19.78s, Speedup=1.00x\n",
      "BS=512, GPUs=2: 10.57s, Speedup=1.87x\n",
      "BS=512, GPUs=4: 10.11s, Speedup=1.96x\n",
      "\n",
      "=== Part 3: Compute vs Communication breakdown ===\n",
      "\n",
      "=== Part 4: Bandwidth utilization ===\n",
      "Model size: 11,173,962 parameters = 0.0416 GB\n",
      "\n",
      "Table 1: Training Time and Speedup\n",
      "Batch Size 32: {1: {'time': 20.19777274131775, 'speedup': 1.0}, 2: {'time': 58.31253170967102, 'speedup': 0.3463710483688018}, 4: {'time': 76.14964199066162, 'speedup': 0.26523792119462153}}\n",
      "Batch Size 128: {1: {'time': 17.73724102973938, 'speedup': 1.0}, 2: {'time': 17.47592544555664, 'speedup': 1.0149528896193123}, 4: {'time': 19.15953230857849, 'speedup': 0.9257658665184486}}\n",
      "Batch Size 512: {1: {'time': 19.77684736251831, 'speedup': 1.0}, 2: {'time': 10.57288932800293, 'speedup': 1.8705243901625024}, 4: {'time': 10.108532428741455, 'speedup': 1.9564508994685583}}\n",
      "\n",
      "Table 2: Compute vs Communication Breakdown\n",
      "Batch Size 32: {2: {'compute': 20.145222902297974, 'communication': 38.16730880737305, 'total': 58.31253170967102}, 4: {'compute': 20.145222902297974, 'communication': 56.00441908836365, 'total': 76.14964199066162}}\n",
      "Batch Size 128: {2: {'compute': 17.627889394760132, 'communication': -0.1519639492034912, 'total': 17.47592544555664}, 4: {'compute': 17.627889394760132, 'communication': 1.5316429138183594, 'total': 19.15953230857849}}\n",
      "Batch Size 512: {2: {'compute': 19.66075325012207, 'communication': -9.08786392211914, 'total': 10.57288932800293}, 4: {'compute': 19.66075325012207, 'communication': -9.552220821380615, 'total': 10.108532428741455}}\n",
      "\n",
      "Table 3: Bandwidth Utilization\n",
      "Batch Size 32: {2: {'T_measured': 38.16730880737305, 'T_theoretical': 0.00013875417411327362, 'B_effective': 0.001090625813941088, 'B_peak': 300, 'utilization': 0.00036354193798036265}, 4: {'T_measured': 56.00441908836365, 'T_theoretical': 0.00020813126116991042, 'B_effective': 0.0011149009197373588, 'B_peak': 300, 'utilization': 0.00037163363991245295}}\n",
      "Batch Size 128: {2: {'T_measured': -0.1519639492034912, 'T_theoretical': 0.00013875417411327362, 'B_effective': -0.273921890370468, 'B_peak': 300, 'utilization': -0.091307296790156}, 4: {'T_measured': 1.5316429138183594, 'T_theoretical': 0.00020813126116991042, 'B_effective': 0.0407662763870417, 'B_peak': 300, 'utilization': 0.013588758795680568}}\n",
      "Batch Size 512: {2: {'T_measured': -9.08786392211914, 'T_theoretical': 0.00013875417411327362, 'B_effective': -0.004580422043145594, 'B_peak': 300, 'utilization': -0.0015268073477151978}, 4: {'T_measured': -9.552220821380615, 'T_theoretical': 0.00020813126116991042, 'B_effective': -0.006536634728043123, 'B_peak': 300, 'utilization': -0.002178878242681041}}\n"
     ]
    }
   ],
   "source": [
    "# === Full Benchmarking Pipeline ===\n",
    "\n",
    "# 1. Measure single GPU training times for different batch sizes\n",
    "print(\"=== Part 1: Single GPU with varying batch sizes ===\")\n",
    "single_gpu_results = measure_single_gpu_training([32, 128, 512])\n",
    "\n",
    "# 2. Measure multi-GPU training and speedup\n",
    "print(\"\\n=== Part 2: Multi-GPU training and speedup ===\")\n",
    "batch_sizes = list(single_gpu_results.keys())\n",
    "multi_gpu_results = {}\n",
    "for bs in batch_sizes:\n",
    "    multi_gpu_results[bs] = {}\n",
    "    for num_gpus in [1, 2, 4]:\n",
    "        time_taken = measure_multi_gpu_training(bs, num_gpus)\n",
    "        multi_gpu_results[bs][num_gpus] = {\n",
    "            'time': time_taken,\n",
    "            'speedup': multi_gpu_results[bs][1]['time'] / time_taken if num_gpus > 1 else 1.0\n",
    "        }\n",
    "        print(f\"BS={bs}, GPUs={num_gpus}: {time_taken:.2f}s, Speedup={multi_gpu_results[bs][num_gpus]['speedup']:.2f}x\")\n",
    "\n",
    "# 3. Compute vs Communication breakdown\n",
    "print(\"\\n=== Part 3: Compute vs Communication breakdown ===\")\n",
    "breakdown = calculate_compute_communication_breakdown(multi_gpu_results, single_gpu_results)\n",
    "\n",
    "# 4. Bandwidth utilization\n",
    "print(\"\\n=== Part 4: Bandwidth utilization ===\")\n",
    "net = resnet.ResNet18()\n",
    "bandwidth_results = calculate_bandwidth_utilization(net, breakdown, [2, 4])\n",
    "\n",
    "# Print summary tables (you can format these as needed)\n",
    "print(\"\\nTable 1: Training Time and Speedup\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Batch Size {bs}: {multi_gpu_results[bs]}\")\n",
    "\n",
    "print(\"\\nTable 2: Compute vs Communication Breakdown\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Batch Size {bs}: {breakdown[bs]}\")\n",
    "\n",
    "print(\"\\nTable 3: Bandwidth Utilization\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Batch Size {bs}: {bandwidth_results[bs]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb3d6e-2fea-4305-a46e-c0d2da326757",
   "metadata": {},
   "source": [
    "The output for the 4 points is the following:\n",
    "=== Part 1: Single GPU with varying batch sizes ===\n",
    "Batch size 32: 20.15s\n",
    "Batch size 128: 17.63s\n",
    "Batch size 512: 19.66s\n",
    "\n",
    "=== Part 2: Multi-GPU training and speedup ===\n",
    "BS=32, GPUs=1: 20.20s, Speedup=1.00x\n",
    "BS=32, GPUs=2: 58.31s, Speedup=0.35x\n",
    "BS=32, GPUs=4: 76.15s, Speedup=0.27x\n",
    "BS=128, GPUs=1: 17.74s, Speedup=1.00x\n",
    "BS=128, GPUs=2: 17.48s, Speedup=1.01x\n",
    "BS=128, GPUs=4: 19.16s, Speedup=0.93x\n",
    "BS=512, GPUs=1: 19.78s, Speedup=1.00x\n",
    "BS=512, GPUs=2: 10.57s, Speedup=1.87x\n",
    "BS=512, GPUs=4: 10.11s, Speedup=1.96x\n",
    "\n",
    "=== Part 3: Compute vs Communication breakdown ===\n",
    "\n",
    "=== Part 4: Bandwidth utilization ===\n",
    "Model size: 11,173,962 parameters = 0.0416 GB\n",
    "\n",
    "Table 1: Training Time and Speedup\n",
    "Batch Size 32: {1: {'time': 20.19777274131775, 'speedup': 1.0}, 2: {'time': 58.31253170967102, 'speedup': 0.3463710483688018}, 4: {'time': 76.14964199066162, 'speedup': 0.26523792119462153}}\n",
    "Batch Size 128: {1: {'time': 17.73724102973938, 'speedup': 1.0}, 2: {'time': 17.47592544555664, 'speedup': 1.0149528896193123}, 4: {'time': 19.15953230857849, 'speedup': 0.9257658665184486}}\n",
    "Batch Size 512: {1: {'time': 19.77684736251831, 'speedup': 1.0}, 2: {'time': 10.57288932800293, 'speedup': 1.8705243901625024}, 4: {'time': 10.108532428741455, 'speedup': 1.9564508994685583}}\n",
    "\n",
    "Table 2: Compute vs Communication Breakdown\n",
    "Batch Size 32: {2: {'compute': 20.145222902297974, 'communication': 38.16730880737305, 'total': 58.31253170967102}, 4: {'compute': 20.145222902297974, 'communication': 56.00441908836365, 'total': 76.14964199066162}}\n",
    "Batch Size 128: {2: {'compute': 17.627889394760132, 'communication': -0.1519639492034912, 'total': 17.47592544555664}, 4: {'compute': 17.627889394760132, 'communication': 1.5316429138183594, 'total': 19.15953230857849}}\n",
    "Batch Size 512: {2: {'compute': 19.66075325012207, 'communication': -9.08786392211914, 'total': 10.57288932800293}, 4: {'compute': 19.66075325012207, 'communication': -9.552220821380615, 'total': 10.108532428741455}}\n",
    "\n",
    "Table 3: Bandwidth Utilization\n",
    "Batch Size 32: {2: {'T_measured': 38.16730880737305, 'T_theoretical': 0.00013875417411327362, 'B_effective': 0.001090625813941088, 'B_peak': 300, 'utilization': 0.00036354193798036265}, 4: {'T_measured': 56.00441908836365, 'T_theoretical': 0.00020813126116991042, 'B_effective': 0.0011149009197373588, 'B_peak': 300, 'utilization': 0.00037163363991245295}}\n",
    "Batch Size 128: {2: {'T_measured': -0.1519639492034912, 'T_theoretical': 0.00013875417411327362, 'B_effective': -0.273921890370468, 'B_peak': 300, 'utilization': -0.091307296790156}, 4: {'T_measured': 1.5316429138183594, 'T_theoretical': 0.00020813126116991042, 'B_effective': 0.0407662763870417, 'B_peak': 300, 'utilization': 0.013588758795680568}}\n",
    "Batch Size 512: {2: {'T_measured': -9.08786392211914, 'T_theoretical': 0.00013875417411327362, 'B_effective': -0.004580422043145594, 'B_peak': 300, 'utilization': -0.0015268073477151978}, 4: {'T_measured': -9.552220821380615, 'T_theoretical': 0.00020813126116991042, 'B_effective': -0.006536634728043123, 'B_peak': 300, 'utilization': -0.002178878242681041}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa96db3-4815-4e10-b7ba-fa82a91adcd6",
   "metadata": {},
   "source": [
    "For part 2 here is the text explanation:\n",
    "Scaling Type: This experiment measures strong scaling because the batch size per GPU remains constant (32, 128, 512, 2048) while the number of GPUs increases, resulting in a proportionally larger total batch size processed per training step.\n",
    "\n",
    "Comparison with Weak Scaling: If weak scaling were used instead (keeping total batch size constant and reducing per-GPU batch size), the speedup would be worse because:​\n",
    "\n",
    "Smaller per-GPU batches reduce GPU compute efficiency\n",
    "\n",
    "Communication overhead (gradient synchronization) remains constant but becomes a larger fraction of total time\n",
    "\n",
    "GPU memory bandwidth and compute resources are underutilized with smaller batches\n",
    "\n",
    "Strong scaling is more appropriate for this deep learning scenario because it maintains high GPU utilization and maximizes throughput per training iteration.\n",
    "\n",
    "| Batch Size per GPU | 1 GPU Time (s) | 2 GPU Time (s) | 2 GPU Speedup | 4 GPU Time (s) | 4 GPU Speedup |\n",
    "|--------------------|----------------|----------------|---------------|----------------|---------------|\n",
    "| 32                 | 20.20          | 58.31          | 0.35          | 76.15          | 0.27          |\n",
    "| 128                | 17.74          | 17.48          | 1.01          | 19.16          | 0.93          |\n",
    "| 512                | 19.78          | 10.57          | 1.87          | 10.11          | 1.96          |\n",
    "\n",
    "\n",
    "\n",
    "For part 3:\n",
    "For each batch size per GPU (e.g., 32, 128, 512), you first measure the training time for one epoch on a single GPU. This time includes all computation (forward pass, backward pass, optimizer step, and CPU-GPU data transfer).\n",
    "\n",
    "For 2-GPU and 4-GPU setups, you measure the total training time for one epoch with the same batch size per GPU.\n",
    "\n",
    "Compute time is assumed to be the same as the single GPU time for that batch size, since each GPU does the same amount of work.\n",
    "\n",
    "Communication time is the extra time spent synchronizing gradients between GPUs. You calculate it by subtracting the single GPU time from the multi-GPU time:\n",
    "\n",
    "\n",
    "Communication Time=Multi-GPU Time−Single GPU Time\n",
    "\n",
    "| Batch Size per GPU | 2 GPU Compute (s) | 2 GPU Comm (s) | 2 GPU Total (s) | 4 GPU Compute (s) | 4 GPU Comm (s) | 4 GPU Total (s) |\n",
    "|--------------------|-------------------|----------------|-----------------|-------------------|----------------|-----------------|\n",
    "| 32                 | 20.15             | 38.17          | 58.31           | 20.15             | 56.00          | 76.15           |\n",
    "| 128                | 17.63             | -0.15          | 17.48           | 17.63             | 1.53           | 19.16           |\n",
    "| 512                | 19.66             | -9.09          | 10.57           | 19.66             | -9.55          | 10.11           |\n",
    "\n",
    "\n",
    "\n",
    "For part 4:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adde3c-47fa-46e5-8239-36ff9016020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage during training\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i} Memory Allocated: {torch.cuda.memory_allocated(i)/1024**3:.2f} GB\")\n",
    "        print(f\"GPU {i} Memory Cached: {torch.cuda.memory_reserved(i)/1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea7c9f-41e1-47cc-9159-cd8c9d0817a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(train_accs, label='Train Acc')\n",
    "ax2.plot(test_accs, label='Test Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_progress.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
